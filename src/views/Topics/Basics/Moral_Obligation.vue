<template>
  <div  id="Moral_Obligation" class="TopicExplanation" >
    <div class="TopicHeading">Moral Obligation</div>
<P>
With our discussion in Chapter 2, we now have the tools to discuss moral obligation (this chapter) and moral responsibility (the next chapter).  What does moral obligaton mean?  Indeed, what does any sort of obligation mean?  
</p>
<h2>
Obligation
</h2>
<p>
Before we tackle moral obligation, we need to know what we mean by obligation writ large.  The problem with the concept of obligation in the deterministic world is that everything that we do is determined.  In a sense, we are obligated by causal forces to do everything that we do.  But if every action is obligated, then the concept of obligation loses its meaning and usefulness. If “to obligate” just means “to cause,” then the term “obligate” adds nothing to our vocabulary.  Therefore, in order for the concept of obligation to be useful, it cannot just refer to actions that are caused.
</p>
<h2>
Legal Obligation
</h2>
<p>
Fortunately, obligation has other meanings that make much more sense in a deterministic world.  As an example, consider legal obligation as expressed in this statement:
</p>
<p style="margin-left:20px;">
You have a legal obligation to file your taxes by the deadline.
</p>
<p>
This seems simple enough.  The word legal refers to the government.  This says that if you do not file your taxes by the deadline, then there will be legal consequences.  That is, you may be required to pay a fine, and if you are particularly stubborn, you might even find yourself in jail.  Note that in spite of these consequences, you may still choose not to file your taxes by the deadline and accept the consequences.  Therefore, if we define obligation as simply a specific type of conditional statement, then it makes perfect sense in a deterministic world.  We could rewrite the above statement, for example, as:
</p>
<p style="margin-left:20px;">
If you do not file your taxes on time, the government will fine you $1000.
</p>
<p>
What makes this a legal obligation is that the first part of the conditional statement refers to an action (or inaction) and the second part of the conditional statement is a government imposed sanction.  Thusly defined, we can note the following about this sort of obligation:
</p>
<p style="margin-left:20px;">
1.  Not all acts are subject to a legal obligation, and
</p>
<p style="margin-left:20px;">
2.  Those acts subject to a legal obligation may nevertheless not happen.
</p>
<p>
Thus, legal obligation means something quite different from causation.  
</p>
<h2>
Moral Obligation
</h2>
<p>
Moral obligation also makes sense in a deterministic world.  Consider the following statement:
</p>
<p style="margin-left:20px;">
You have a moral obligation to contribute to the support of your children.
</p>
<p>
Note that you may or may not have a legal obligation to care for your children, depending on the circumstances, but in either case someone could make the claim that you have a moral obligation to do so.  Religious and philosophical traditions posit all sorts of moral obligations, from the Ten Commandments to the Golden Rule to the Eight Fold Path to the Ten Vedic Restraints to the precepts of Confucius and on and on.  Such moral obligations are typically not expressed conditionally, but rather as commands:
</p>
<p style="margin-left:20px;">
Thou shall not kill.
</p>
<p style="margin-left:20px;">
Hate not.
</p>
<p style="margin-left:20px;">
Practice noninjury.
</p>
<p>
As we noted in Chapter 2, commands make perfect sense in a deterministic universe.  We could, for example, program our robots with various moral obligations.  (Again, note Asimov’s Three Laws of Robotics.)  
</p>
<h2>
Moral Rules
</h2>
<p>
We typically express moral obligation as moral rules.  Moral rules specify certain types of behavior as good and other types as bad.  Moral philosophy, at its root, seeks to identify what sorts of behaviors are good and what sorts of behaviors are bad.  
</p>
<p>
There are a number of sources of moral rules.  There are those who believe that the question of good and evil is answered by canonical texts.  Good and bad behavior is what the Bible, the Quran, the Book of Morman, or the Hindu texts say they are.  Or perhaps they are expressed by the utterances of a charismatic or spiritual leader.
</p>
<p>
Afreeism does not answer the question of what behavior is good or what behavior is bad.  Because afreeism is based on science rather than faith, its adherents are more likely to take a nondogmatic approach.  That is, afreeists, are likely to define good behaviors as those that promote human flourishing and bad behaviors as those that cause human suffering.  Some candidates for moral rules along these lines might include some of the rules our parents taught us:
</p>
<p style="margin-left:20px;">
Reciprocity (also known as the Golden Rule):  Treat others as you would like to be treated.  (Parent: “How would you feel is someone did that to you?”)  Versions of this rule date at least from the time of Confucius.
</p>
<p style="margin-left:20px;">
Universality (also known as Kant’s Categorical Imperative):  Adopt a moral rule if and only if you would have that moral rule become a universal law.  (Parent:  “What if everyone acted like that?”) 
</p>
<p>
To this, I would add one more:
</p>
<p style="margin-left:20px;">
The Veil of Ignorance:  This idea was developed by the philosopher John Rawls.   It goes like this:  in adopting a moral (societal) rule, pretend that you do not know which member of society you will be.  Would you adopt this rule under these circumstances?  (This is sort of like the I-cut-you-choose rule for divvying up desserts.)
</p>
<p>
Of course, these rules do not answer many of the hard moral questions.  Afreeists must struggle with these as do other nondogmatic thinkers.  
</p>
<p>
Moral rules are completely compatible with determinism.   As we saw in Robot World, we could program our robots to follow moral rules.  Or we could program them to create their own.  Most likely, the rules we would choose would be ones that allow the society to function and its inhabitants to flourish.  
</p>
<p>
Indeed, if we are clever enough and good enough programmers, we could program our robots to create their own internal commands.  For example, we could program our robots to adopt internal commands that satisfy a specified criterion.  Here is an example of a criterion:
</p>
<p style="margin-left:20px;">
A robot will adopt an internal command if the robot determines that Robot World would be better if all robots adopted the command.
</p>
<p>
Of course, we would have to program the robot to know what “better” meant in the above question.  This would mean having some sort of societal welfare function that our robots could refer to.
</p>
<h2>
Moral Obligation and Afreeism
</h2>
<p>
Moral obligation makes perfect sense in a deterministic universe.  If we were designing Robot World, we would most likely want to program into our robots moral commands.  Moral obligation would likely make robot society function better.  Likewise for human society.  Moral obligation is completely compatible with afreeism.  Not so for moral responsibility.  That is the topic of the next chapter.
</p>


        <p><a href="#Back">Back to top.</a></p>
    </div>
</template>

<script>
// @ is an alias to /src

export default {
  name: 'Moral_Obligation',
};
</script>
<style scoped>

</style>
